{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 05 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tWhat are Sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441c087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tWhat are the Problem with Vanilla RNNs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afab6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tWhat is Gradient clipping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d345f5",
   "metadata": {},
   "source": [
    "**Ans:** Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. A neural network is a learning algorithm, also called neural network or neural net, that uses a network of functions to understand and translate data input into a specific output. \n",
    "\n",
    "This type of learning algorithm is designed based on the way neurons function in the human brain. There are many ways to compute gradient clipping, but a common one is to rescale gradients so that their norm is at most a particular value. With gradient clipping, pre-determined gradient threshold be introduced, and  then gradients norms that exceed this threshold are scaled down to match the norm.  This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped.  There is an introduced bias in the resulting values from the gradient, but gradient clipping can keep things stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tExplain Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e063e3",
   "metadata": {},
   "source": [
    "**Ans:** The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation. The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all of the encoded input vectors, with the most relevant vectors being attributed the highest weights. \n",
    "\n",
    "The attention mechanism was introduced by Bahdanau et al. (2014), to address the bottleneck problem that arises with the use of a fixed-length encoding vector, where the decoder would have limited access to the information provided by the input. This is thought to become especially problematic for long and/or complex sequences, where the dimensionality of their representation would be forced to be the same as for shorter or simpler sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tExplain Conditional random fields (CRFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb423a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tExplain self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e01191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "601b0899",
   "metadata": {},
   "source": [
    "#### 7.\tWhat is Bahdanau Attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b6e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba4026d",
   "metadata": {},
   "source": [
    "#### 8.\tWhat is a Language Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e04bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc53fbdb",
   "metadata": {},
   "source": [
    "#### 9.\tWhat is Multi-Head Attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c0d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0abe2b7a",
   "metadata": {},
   "source": [
    "#### 10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae414d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
